{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Spark I/O with `pyspark.sql` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SparkContext` and `SparkSession`\n",
    "\n",
    "For a pyspark to run, a `sc` is needed to establish a connection to the Spark cluster (similar to a `sqlalchemy` database Engine), while a `spark` session is needed to **interact** with `sc`. Often times, there's a need for user to ensure only **1** `sc` and `spark` exist when running to avoid serious bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SparkConf` is used to define the meta-settings for a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('SparkBeginner')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.version` indicates the specific Spark version of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=SparkBeginner>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `pyspark.sql` to interact with PySpark DataFrame\n",
    "It is often recommended to use the `pyspark.sql` API to interact with **PySpark DataFrame**, which is a higher level API than the operations around Spark RDDs. To use the `pyspark.sql` API, a `SparkSession` is always needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .getOrCreate() method of the builder will only create a new \n",
    "# spark session unless there's none such session existing\n",
    "my_spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f75d85d8cc0>\n"
     ]
    }
   ],
   "source": [
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `SparkSession` has an attribute called `catalog` which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. One of the most useful is the `.listTables()` method, which returns the names of all the tables in your cluster as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save Data into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a csv table into Spark local view. The `spark.read.methods()` provides an API to reading different file formats into `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = my_spark.read.csv('./input/ml-latest-small/links.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the other movielens tables into spark context\n",
    "movies_df = my_spark.read.csv('./input/ml-latest-small/movies.csv', header=True)\n",
    "ratings_df = my_spark.read.csv('./input/ml-latest-small/ratings.csv', header=True)\n",
    "tags_df = my_spark.read.csv('./input/ml-latest-small/tags.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|movieId| imdbId|tmdbId|\n",
      "+-------+-------+------+\n",
      "|      1|0114709|   862|\n",
      "|      2|0113497|  8844|\n",
      "|      3|0113228| 15602|\n",
      "|      4|0114885| 31357|\n",
      "|      5|0113041| 11862|\n",
      "|      6|0113277|   949|\n",
      "|      7|0114319| 11860|\n",
      "|      8|0112302| 45325|\n",
      "|      9|0114576|  9091|\n",
      "|     10|0113189|   710|\n",
      "|     11|0112346|  9087|\n",
      "|     12|0112896| 12110|\n",
      "|     13|0112453| 21032|\n",
      "|     14|0113987| 10858|\n",
      "|     15|0112760|  1408|\n",
      "|     16|0112641|   524|\n",
      "|     17|0114388|  4584|\n",
      "|     18|0113101|     5|\n",
      "|     19|0112281|  9273|\n",
      "|     20|0113845| 11517|\n",
      "+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.createOrReplaceTempView('table_name')` will create a table name inside the SparkContext. **Notice that until creating a \"View\", the imported data will not show up in `spark.catalog.listTables()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.createOrReplaceTempView('links')\n",
    "movies_df.createOrReplaceTempView('movies')\n",
    "ratings_df.createOrReplaceTempView('ratings')\n",
    "tags_df.createOrReplaceTempView('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='links', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='movies', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='ratings', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='tags', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this dataframe via `pandas`, simply call `.toPandas()` method on the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_pd = links_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0114709</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0113497</td>\n",
       "      <td>8844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0113228</td>\n",
       "      <td>15602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0114885</td>\n",
       "      <td>31357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0113041</td>\n",
       "      <td>11862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9737</td>\n",
       "      <td>193581</td>\n",
       "      <td>5476944</td>\n",
       "      <td>432131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9738</td>\n",
       "      <td>193583</td>\n",
       "      <td>5914996</td>\n",
       "      <td>445030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9739</td>\n",
       "      <td>193585</td>\n",
       "      <td>6397426</td>\n",
       "      <td>479308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9740</td>\n",
       "      <td>193587</td>\n",
       "      <td>8391976</td>\n",
       "      <td>483455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9741</td>\n",
       "      <td>193609</td>\n",
       "      <td>0101726</td>\n",
       "      <td>37891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     movieId   imdbId  tmdbId\n",
       "0          1  0114709     862\n",
       "1          2  0113497    8844\n",
       "2          3  0113228   15602\n",
       "3          4  0114885   31357\n",
       "4          5  0113041   11862\n",
       "...      ...      ...     ...\n",
       "9737  193581  5476944  432131\n",
       "9738  193583  5914996  445030\n",
       "9739  193585  6397426  479308\n",
       "9740  193587  8391976  483455\n",
       "9741  193609  0101726   37891\n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can easily turn any pandas dataframe into a spark dataframe with `spark.createDataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({'X': [1, 2, 3],\n",
    "                        'y': [0, 0, 1]})\n",
    "temp_spdf = my_spark.createDataFrame(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  X|  y|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  2|  0|\n",
      "|  3|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_spdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a table from Spark, simply call `spark.table('table_name')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = my_spark.table('links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|movieId| imdbId|tmdbId|\n",
      "+-------+-------+------+\n",
      "|      1|0114709|   862|\n",
      "|      2|0113497|  8844|\n",
      "|      3|0113228| 15602|\n",
      "|      4|0114885| 31357|\n",
      "|      5|0113041| 11862|\n",
      "|      6|0113277|   949|\n",
      "|      7|0114319| 11860|\n",
      "|      8|0112302| 45325|\n",
      "|      9|0114576|  9091|\n",
      "|     10|0113189|   710|\n",
      "|     11|0112346|  9087|\n",
      "|     12|0112896| 12110|\n",
      "|     13|0112453| 21032|\n",
      "|     14|0113987| 10858|\n",
      "|     15|0112760|  1408|\n",
      "|     16|0112641|   524|\n",
      "|     17|0114388|  4584|\n",
      "|     18|0113101|     5|\n",
      "|     19|0112281|  9273|\n",
      "|     20|0113845| 11517|\n",
      "+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Manipulation with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `spark_df.withColumn()` to conduct column-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column-wise operations, use the `spark_dataframe.withColumn()` method that takes two arguments:\n",
    "\n",
    "1. first arg, a string with the name of your new column\n",
    "2. second arg, the new column itself (i.e. the calculation)\n",
    "\n",
    "The new column in arg 2 must be an object of class `Column`. Creating one of these is as easy as extracting a column from your DataFrame using `spark_dataframe.colName`.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "Updating a Spark DataFrame is different than working in pandas because the Spark DataFrame is ***immutable***. This means that it can't be changed, and so columns can't be updated in place. Instead, all the operations methods will return a new DataFrame.\n",
    "\n",
    "In order to overwrite the original DataFrame, we must reassign the returned DataFrame by assigning it back to the original spark dataframe name:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"newCol\", df.oldCol * 2)\n",
    "```\n",
    "\n",
    "The above code creates a DataFrame with the same columns as `df` **AND** a new column, newCol, where every entry is equal to the corresponding entry from oldCol multiplied by 2.\n",
    "\n",
    "To overwrite an existing column, just pass the name of the column as the first argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = my_spark.table('ratings')\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|userId|movieId|rating|timestamp|rating_int|\n",
      "+------+-------+------+---------+----------+\n",
      "|     1|      1|   4.0|964982703|         4|\n",
      "|     1|      3|   4.0|964981247|         4|\n",
      "|     1|      6|   4.0|964982224|         4|\n",
      "|     1|     47|   5.0|964983815|         5|\n",
      "|     1|     50|   5.0|964982931|         5|\n",
      "|     1|     70|   3.0|964982400|         3|\n",
      "|     1|    101|   5.0|964980868|         5|\n",
      "|     1|    110|   4.0|964982176|         4|\n",
      "|     1|    151|   5.0|964984041|         5|\n",
      "|     1|    157|   5.0|964984100|         5|\n",
      "|     1|    163|   5.0|964983650|         5|\n",
      "|     1|    216|   5.0|964981208|         5|\n",
      "|     1|    223|   3.0|964980985|         3|\n",
      "|     1|    231|   5.0|964981179|         5|\n",
      "|     1|    235|   4.0|964980908|         4|\n",
      "|     1|    260|   5.0|964981680|         5|\n",
      "|     1|    296|   3.0|964982967|         3|\n",
      "|     1|    316|   3.0|964982310|         3|\n",
      "|     1|    333|   5.0|964981179|         5|\n",
      "|     1|    349|   4.0|964982563|         4|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change the data format of the 'rating' column from float to int\n",
    "ratings = ratings.withColumn('rating_int', ratings['rating'].astype('int'))\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `spark_df.filter()` to filter rows\n",
    "\n",
    "The `.filter()` method takes in 2 formats for filtering:\n",
    "\n",
    "1. A SQL string will work if it works on an equivalent `WHERE...` SQL statment clause\n",
    "2. A boolean `Column` will work, similar to how normally a pandas selector works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|rating_int|\n",
      "+------+-------+------+----------+----------+\n",
      "|     1|      1|   4.0| 964982703|         4|\n",
      "|     5|      1|   4.0| 847434962|         4|\n",
      "|     7|      1|   4.5|1106635946|         4|\n",
      "|    15|      1|   2.5|1510577970|         2|\n",
      "|    17|      1|   4.5|1305696483|         4|\n",
      "|    18|      1|   3.5|1455209816|         3|\n",
      "|    19|      1|   4.0| 965705637|         4|\n",
      "|    21|      1|   3.5|1407618878|         3|\n",
      "|    27|      1|   3.0| 962685262|         3|\n",
      "|    31|      1|   5.0| 850466616|         5|\n",
      "|    32|      1|   3.0| 856736119|         3|\n",
      "|    33|      1|   3.0| 939647444|         3|\n",
      "|    40|      1|   5.0| 832058959|         5|\n",
      "|    43|      1|   5.0| 848993983|         5|\n",
      "|    44|      1|   3.0| 869251860|         3|\n",
      "|    45|      1|   4.0| 951170182|         4|\n",
      "|    46|      1|   5.0| 834787906|         5|\n",
      "|    50|      1|   3.0|1514238116|         3|\n",
      "|    54|      1|   3.0| 830247330|         3|\n",
      "|    57|      1|   5.0| 965796031|         5|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|rating_int|\n",
      "+------+-------+------+----------+----------+\n",
      "|     1|      1|   4.0| 964982703|         4|\n",
      "|     5|      1|   4.0| 847434962|         4|\n",
      "|     7|      1|   4.5|1106635946|         4|\n",
      "|    15|      1|   2.5|1510577970|         2|\n",
      "|    17|      1|   4.5|1305696483|         4|\n",
      "|    18|      1|   3.5|1455209816|         3|\n",
      "|    19|      1|   4.0| 965705637|         4|\n",
      "|    21|      1|   3.5|1407618878|         3|\n",
      "|    27|      1|   3.0| 962685262|         3|\n",
      "|    31|      1|   5.0| 850466616|         5|\n",
      "|    32|      1|   3.0| 856736119|         3|\n",
      "|    33|      1|   3.0| 939647444|         3|\n",
      "|    40|      1|   5.0| 832058959|         5|\n",
      "|    43|      1|   5.0| 848993983|         5|\n",
      "|    44|      1|   3.0| 869251860|         3|\n",
      "|    45|      1|   4.0| 951170182|         4|\n",
      "|    46|      1|   5.0| 834787906|         5|\n",
      "|    50|      1|   3.0|1514238116|         3|\n",
      "|    54|      1|   3.0| 830247330|         3|\n",
      "|    57|      1|   5.0| 965796031|         5|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below 2 calls return the same results\n",
    "ratings.filter('movieId == 1').show()\n",
    "ratings.filter(ratings.movieId == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `spark_df.select()` to **cherry pick** columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark variant of SQL's `SELECT` is the `spark_df.select()` method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the `df.colName` syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside `.withColumn()`.\n",
    "\n",
    "**NOTE:**\n",
    "The difference between `.select()` and `.withColumn()` methods is that `.select()` returns ***only the columns you specify***, while `.withColumn()` returns ***all the columns of the DataFrame in addition to the one you defined***.\n",
    "\n",
    "It's often a good idea to drop columns you don't need at the beginning of an operation so that you're not dragging around extra data as you're wrangling. In this case, you would use `.select()` and not `.withColumn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      1|   4.0|\n",
      "|     1|      3|   4.0|\n",
      "|     1|      6|   4.0|\n",
      "|     1|     47|   5.0|\n",
      "|     1|     50|   5.0|\n",
      "|     1|     70|   3.0|\n",
      "|     1|    101|   5.0|\n",
      "|     1|    110|   4.0|\n",
      "|     1|    151|   5.0|\n",
      "|     1|    157|   5.0|\n",
      "|     1|    163|   5.0|\n",
      "|     1|    216|   5.0|\n",
      "|     1|    223|   3.0|\n",
      "|     1|    231|   5.0|\n",
      "|     1|    235|   4.0|\n",
      "|     1|    260|   5.0|\n",
      "|     1|    296|   3.0|\n",
      "|     1|    316|   3.0|\n",
      "|     1|    333|   5.0|\n",
      "|     1|    349|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      1|   4.0|\n",
      "|     1|      3|   4.0|\n",
      "|     1|      6|   4.0|\n",
      "|     1|     47|   5.0|\n",
      "|     1|     50|   5.0|\n",
      "|     1|     70|   3.0|\n",
      "|     1|    101|   5.0|\n",
      "|     1|    110|   4.0|\n",
      "|     1|    151|   5.0|\n",
      "|     1|    157|   5.0|\n",
      "|     1|    163|   5.0|\n",
      "|     1|    216|   5.0|\n",
      "|     1|    223|   3.0|\n",
      "|     1|    231|   5.0|\n",
      "|     1|    235|   4.0|\n",
      "|     1|    260|   5.0|\n",
      "|     1|    296|   3.0|\n",
      "|     1|    316|   3.0|\n",
      "|     1|    333|   5.0|\n",
      "|     1|    349|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below 2 sets of codes selects the same results\n",
    "ratings.select('userId', 'movieId', 'rating').show()\n",
    "ratings.select(ratings.userId, ratings.movieId, ratings.rating).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+---------+----------+\n",
      "|userId|movieId|movieRating|timestamp|rating_int|\n",
      "+------+-------+-----------+---------+----------+\n",
      "|     1|      1|        4.0|964982703|         4|\n",
      "|     1|      3|        4.0|964981247|         4|\n",
      "|     1|      6|        4.0|964982224|         4|\n",
      "|     1|     47|        5.0|964983815|         5|\n",
      "|     1|     50|        5.0|964982931|         5|\n",
      "|     1|     70|        3.0|964982400|         3|\n",
      "|     1|    101|        5.0|964980868|         5|\n",
      "|     1|    110|        4.0|964982176|         4|\n",
      "|     1|    151|        5.0|964984041|         5|\n",
      "|     1|    157|        5.0|964984100|         5|\n",
      "|     1|    163|        5.0|964983650|         5|\n",
      "|     1|    216|        5.0|964981208|         5|\n",
      "|     1|    223|        3.0|964980985|         3|\n",
      "|     1|    231|        5.0|964981179|         5|\n",
      "|     1|    235|        4.0|964980908|         4|\n",
      "|     1|    260|        5.0|964981680|         5|\n",
      "|     1|    296|        3.0|964982967|         3|\n",
      "|     1|    316|        3.0|964982310|         3|\n",
      "|     1|    333|        5.0|964981179|         5|\n",
      "|     1|    349|        4.0|964982563|         4|\n",
      "+------+-------+-----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use .withColumnRanamed if only column action is to rename column\n",
    "ratings.withColumnRenamed('rating', 'movieRating').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to do column-wise calculation with the `.select()` method. Methodology as below:\n",
    "\n",
    "1. In pythonic API, define the new `Column` by calling the column operations and use `.alias()` to rename the column if needed:\n",
    "```python\n",
    "new_col = (df.old_col1 * 2 - df.old_col2).alias('new_col')\n",
    "```\n",
    "Then use the `.select()` as always:\n",
    "```python\n",
    "selected_df = df.select(df.old_col1, df.old_col2, new_col)\n",
    "```\n",
    "2. Alternatively, use the SQL string that follows `SELECT ... AS ...` protocal:\n",
    "```python\n",
    "selected_df = df.select('old_col1', 'old_col2', 'old_col1 * 2 - old_col2 AS new_col')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `.groupBy()` method to create GroupedData and do aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the common aggregation methods, like `.min()`, `.max()`, and `.count()` are `GroupedData` methods. These are created by calling the `.groupBy()` DataFrame method. For example, to find the minimum value of a column, col, in a DataFrame, df, you could do\n",
    "```python\n",
    "df.groupBy().min(\"col\").show() # notice .groupBy() turns data into GroupedData object\n",
    "```\n",
    "This creates a GroupedData object (so you can use the `.min()` method), then finds the minimum value in col, and returns it as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|min(timestamp)|\n",
      "+--------------+\n",
      "|     828124615|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examining the min timestamp\n",
    "ratings.withColumn(\n",
    "    'timestamp', ratings.timestamp.astype('int')).groupBy().min('timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examining the max rating\n",
    "ratings.select(ratings.rating.astype('int')).alias(\n",
    "    'rating').groupBy().max('rating').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|    296|  4.09771986970684|\n",
      "|   1090| 3.873015873015873|\n",
      "| 115713| 3.642857142857143|\n",
      "|   3210|3.3333333333333335|\n",
      "|  88140|           3.34375|\n",
      "|    829|2.6666666666666665|\n",
      "|   2088| 2.388888888888889|\n",
      "|   2294| 3.088888888888889|\n",
      "|   4821|               2.8|\n",
      "|  48738|               3.8|\n",
      "|   3959|               3.5|\n",
      "|  89864|3.4210526315789473|\n",
      "|   2136|2.2857142857142856|\n",
      "|    691|3.3333333333333335|\n",
      "|   3606|               3.5|\n",
      "| 121007|               4.0|\n",
      "|   6731|              3.25|\n",
      "|  27317|               3.5|\n",
      "|  26082| 4.333333333333333|\n",
      "| 100553|               4.0|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the avg movie rating\n",
    "ratings.withColumn('rating', ratings.rating.astype('int')).groupBy(\n",
    "    'movieId').avg('rating').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   296|   27|\n",
      "|   467|   22|\n",
      "|   125|  360|\n",
      "|   451|   34|\n",
      "|     7|  152|\n",
      "|    51|  359|\n",
      "|   124|   50|\n",
      "|   447|   78|\n",
      "|   591|   54|\n",
      "|   307|  975|\n",
      "|   475|  155|\n",
      "|   574|   23|\n",
      "|   169|  269|\n",
      "|   205|   27|\n",
      "|   334|  154|\n",
      "|   544|   22|\n",
      "|   577|  161|\n",
      "|   581|   40|\n",
      "|   272|   31|\n",
      "|   442|   20|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the count of ratings per user\n",
    "ratings.groupBy('userId').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the GroupedData's built-in methods above, there is also the `.agg()` method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the `pyspark.sql.functions` submodule.\n",
    "\n",
    "This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule **take the name of a column in a GroupedData table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convention to import sql functions\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+----------+\n",
      "|movieId|        avg_rating|         std_rating|user_count|\n",
      "+-------+------------------+-------------------+----------+\n",
      "|   3210|3.4761904761904763| 0.8621610748180847|        42|\n",
      "| 100553|               4.5|                0.0|         2|\n",
      "|   3606|              3.75| 0.8660254037844386|         4|\n",
      "|    296| 4.197068403908795| 0.9519971466349251|       307|\n",
      "|   6731|             3.625|  1.026436275942851|         8|\n",
      "|   2136|2.4642857142857144| 1.1513250846522238|        14|\n",
      "|   1090| 3.984126984126984| 0.9374306596647389|        63|\n",
      "|  48738|             3.975| 0.9101041004655513|        20|\n",
      "|   2088|               2.5|  0.954863710632231|        18|\n",
      "|  89864|3.6315789473684212| 0.5487892041970895|        19|\n",
      "| 115713|3.9107142857142856|  1.178932123737814|        28|\n",
      "|   2294|3.2444444444444445| 0.7659087162424525|        45|\n",
      "|  88140|          3.546875| 0.6642892757944268|        32|\n",
      "| 112911|               2.0| 1.4719601443879744|         4|\n",
      "|   1572|               3.0| 0.7071067811865476|         2|\n",
      "| 102684|              3.75| 0.3535533905932738|         2|\n",
      "|   3414|               4.0|                NaN|         1|\n",
      "|   5325|3.7857142857142856|0.48795003647426655|         7|\n",
      "|    829|2.6666666666666665|                1.0|         9|\n",
      "|  27317|              3.75| 0.5244044240850758|         6|\n",
      "+-------+------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the count of unique user, mean rating, rating standard deviation,\n",
    "# of ratings by movieId\n",
    "ratings.select('userId', 'movieId', ratings.rating.astype('float')).groupBy(\n",
    "    'movieId').agg(F.avg('rating').alias('avg_rating'),\n",
    "                   F.stddev('rating').alias('std_rating'),\n",
    "                   F.countDistinct('userId').alias('user_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `spark_df.join()` to merge additional tables to a spark dataframe\n",
    "\n",
    "The `.join()` method takes three arguments:\n",
    "\n",
    "1. The first is the second spark dataFrame that you want to join with the first one (i.e. the \"right\" dataframe\n",
    "2. The second argument, `on`, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table\n",
    "3. The third argument, `how`, specifies the kind of join to perform\n",
    "\n",
    "For instance:\n",
    "```python\n",
    "merge_df = left_df.join(right_df, 'foreign_key', 'leftouter')\n",
    "```\n",
    "Above code joines the right DataFrame to the left DataFrame by matching the `foreign_key` column values and will keep all records from the left DataFrame in the merged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the movies to the ratings dataframe\n",
    "movies = my_spark.table('movies')\n",
    "movie_rating = ratings.join(movies, 'movieId', 'leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating|timestamp|rating_int|               title|              genres|\n",
      "+-------+------+------+---------+----------+--------------------+--------------------+\n",
      "|      1|     1|   4.0|964982703|         4|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      3|     1|   4.0|964981247|         4|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      6|     1|   4.0|964982224|         4|         Heat (1995)|Action|Crime|Thri...|\n",
      "|     47|     1|   5.0|964983815|         5|Seven (a.k.a. Se7...|    Mystery|Thriller|\n",
      "|     50|     1|   5.0|964982931|         5|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|     70|     1|   3.0|964982400|         3|From Dusk Till Da...|Action|Comedy|Hor...|\n",
      "|    101|     1|   5.0|964980868|         5|Bottle Rocket (1996)|Adventure|Comedy|...|\n",
      "|    110|     1|   4.0|964982176|         4|   Braveheart (1995)|    Action|Drama|War|\n",
      "|    151|     1|   5.0|964984041|         5|      Rob Roy (1995)|Action|Drama|Roma...|\n",
      "|    157|     1|   5.0|964984100|         5|Canadian Bacon (1...|          Comedy|War|\n",
      "|    163|     1|   5.0|964983650|         5|    Desperado (1995)|Action|Romance|We...|\n",
      "|    216|     1|   5.0|964981208|         5|Billy Madison (1995)|              Comedy|\n",
      "|    223|     1|   3.0|964980985|         3|       Clerks (1994)|              Comedy|\n",
      "|    231|     1|   5.0|964981179|         5|Dumb & Dumber (Du...|    Adventure|Comedy|\n",
      "|    235|     1|   4.0|964980908|         4|      Ed Wood (1994)|        Comedy|Drama|\n",
      "|    260|     1|   5.0|964981680|         5|Star Wars: Episod...|Action|Adventure|...|\n",
      "|    296|     1|   3.0|964982967|         3| Pulp Fiction (1994)|Comedy|Crime|Dram...|\n",
      "|    316|     1|   3.0|964982310|         3|     Stargate (1994)|Action|Adventure|...|\n",
      "|    333|     1|   5.0|964981179|         5|    Tommy Boy (1995)|              Comedy|\n",
      "|    349|     1|   4.0|964982563|         4|Clear and Present...|Action|Crime|Dram...|\n",
      "+-------+------+------+---------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_rating.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
