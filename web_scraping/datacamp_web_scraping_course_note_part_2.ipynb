{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Components of a web crawling script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level components of a crawler\n",
    "\n",
    "There are three components of a web crawler script:\n",
    "\n",
    "1.The `import` statements to import modules\n",
    "\n",
    "2.The `scrapy.Spider` class definition to define a specific spider (inheriting from `scarpy.Spider` class)\n",
    "\n",
    "3.The `scrapy.crawler.CrawlerProcess` process that defines how spider should craw the web\n",
    "\n",
    "\n",
    "Sample codes below:\n",
    "\n",
    "```python\n",
    "# section 1, ipmorts\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# section 2, define spider\n",
    "class SpiderClassName(scrapy.Spider):\n",
    "    name = 'spider_name'\n",
    "    # code for your spider\n",
    "\n",
    "# sesction 3, define process to run spider\n",
    "process = CrawlerProcess()\n",
    "\n",
    "process.crawl(YourSpider)\n",
    "\n",
    "process.start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Spider` class\n",
    "\n",
    "For any `Spider` class, 3 components are required:\n",
    "\n",
    "1.A `name` attribute that's needed to internally refer to the object\n",
    "\n",
    "1.A method called `.start_requests()` is required to kick start the site visiting behavior\n",
    "\n",
    "2.A method called `.parse()` is required to parse the returned web contents\n",
    "\n",
    "Example of defining a `Spider` class is as below, where `.start_requests()` kicks off url visits and `parse()` saves the response to local file.\n",
    "\n",
    "```python\n",
    "class WebSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'web_spider'\n",
    "    \n",
    "    # generator function, only creating one Request at a time\n",
    "    def start_requests(self):\n",
    "        urls = ['url_1', 'url_2', ...]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        # define how the exact result should be processed.\n",
    "        # here we just save out\n",
    "        html_file = 'website_page1.html'\n",
    "        with open(html_file, 'wb') as fout:\n",
    "            fout.write(response.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details above regarding the `.start_requests(self)` method**\n",
    "\n",
    "* Use the `yield` call to create a generator function instead of `return`. Couple with a `for` loop to make start_requests() iterate through the url list\n",
    "* For each iterated link, apply `scrapy.Request()` on the URL with at least the following parameters:\n",
    "    * `url`: the URL to be visited in each iteration by the generator\n",
    "    * `callback`: The parsing function with which the yielded response will be processed\n",
    "\n",
    "**Details above regarding the `.parse(self, response)` method**\n",
    "\n",
    "* The `response` parameter will take the yielded `scrapy.Response` object from the `.start_requests()` generator automatically since is was defined as the `callback` from above `scrapy.Request()` class. This `Response` object will keep track of any further `.follow()` calls on new url links stemming from this initial parent URL\n",
    "* The `.parse()` methods can take any method name as long as the `scrapy.Request()` object takes its correct name in the `callback` parameter\n",
    "* The `.parse()` methods (or any further parsing methods) themselves can be generator functions, allowing the spider to go into multiple layers of links if needed. **Notice that for any further requests, we us the existing `Response` object's `.follow()` method instead, so that all the sites visited will be recorded in the `Response` object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Scraping Author Names of DataCamp websites\n",
    "\n",
    "```python\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css('p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Scraping Contents from a URL Stemming from Starting URL\n",
    "\n",
    "```python\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  \n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    # url_short is a global variable defined already\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url=link, callback=self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
